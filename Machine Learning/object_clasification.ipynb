{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From c:\\Users\\bajoe\\anaconda3\\envs\\tensorflow\\Lib\\site-packages\\keras\\src\\losses.py:2976: The name tf.losses.sparse_softmax_cross_entropy is deprecated. Please use tf.compat.v1.losses.sparse_softmax_cross_entropy instead.\n",
      "\n"
     ]
    }
   ],
   "source": [
    "import zipfile\n",
    "import os\n",
    "import random\n",
    "import shutil\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import seaborn as sns\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential, Model, load_model\n",
    "from tensorflow.keras.layers import Conv2D, MaxPooling2D, Flatten, Dense, Dropout, BatchNormalization\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "from tensorflow.keras.preprocessing.image import ImageDataGenerator\n",
    "import matplotlib.pyplot as plt\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ReduceLROnPlateau, ModelCheckpoint\n",
    "from tensorflow.keras.preprocessing import image\n",
    "from tensorflow.keras.applications.vgg16 import VGG16, preprocess_input\n",
    "from tensorflow.keras.applications.resnet50 import ResNet50\n",
    "from sklearn.model_selection import train_test_split\n",
    "from PIL import Image as im\n",
    "from glob import glob\n",
    "import cv2\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "import re\n",
    "from tensorflow import keras\n",
    "from tensorflow.keras import layers"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_path = 'dataset/'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Add class name prefix to filename. So for example \"/paper104.jpg\" become \"paper/paper104.jpg\"\n",
    "def add_class_name_prefix(df, col_name):\n",
    "    df[col_name] = df[col_name].apply(lambda x: x[:re.search(\"\\d\",x).start()] + '/' + x)\n",
    "    return df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{0: 'botol-plastik', 1: 'daun-basah', 2: 'daun-kering', 3: 'kertas-bungkus', 4: 'kertas-koran', 5: 'logam-ferro', 6: 'logam-non-ferro', 7: 'plastik-sampul'}\n",
      "defining constants successful!\n"
     ]
    }
   ],
   "source": [
    "categories = {}\n",
    "i = 0\n",
    "for dirname, _, filenames in os.walk('dataset'):\n",
    "    if filenames:  # Memastikan hanya direktori dengan file yang diproses\n",
    "        categories[i] = os.path.basename(dirname)\n",
    "        i += 1\n",
    "\n",
    "print(categories)\n",
    "print('defining constants successful!')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "number of elements =  1600\n"
     ]
    }
   ],
   "source": [
    "# Add class name prefix to filename. So for example \"/paper104.jpg\" become \"paper/paper104.jpg\"\n",
    "def add_class_name_prefix(df, col_name):\n",
    "    df[col_name] = df[col_name].apply(lambda x: x[:re.search(\"\\d\",x).start()] + '/' + x)\n",
    "    return df\n",
    "\n",
    "# list conatining all the filenames in the dataset\n",
    "filenames_list = []\n",
    "# list to store the corresponding category, note that each folder of the dataset has one class of data\n",
    "categories_list = []\n",
    "\n",
    "for category in categories:\n",
    "    filenames = os.listdir(data_path + categories[category])\n",
    "    filenames_list = filenames_list  + filenames\n",
    "    categories_list = categories_list + [category] * len(filenames)\n",
    "    \n",
    "df = pd.DataFrame({\n",
    "    'filename': filenames_list,\n",
    "    'category': categories_list\n",
    "})\n",
    "\n",
    "df = add_class_name_prefix(df, 'filename')\n",
    "\n",
    "# Shuffle the dataframe\n",
    "df = df.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "print('number of elements = ' , len(df))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# see sample image, you can run the same cell again to get a different image\n",
    "random_row = random.randint(0, len(df)-1)\n",
    "sample = df.iloc[random_row]\n",
    "randomimage = tf.keras.utils.load_img(data_path +sample['filename'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_visualization = df.copy()\n",
    "# Change the catgegories from numbers to names\n",
    "df_visualization['category'] = df_visualization['category'].apply(lambda x:categories[x] )\n",
    "\n",
    "df_visualization['category'].value_counts().plot.bar(x = 'count', y = 'category' )\n",
    "\n",
    "plt.xlabel(\"Garbage Classes\", labelpad=14)\n",
    "plt.ylabel(\"Images Count\", labelpad=14)\n",
    "plt.title(\"Count of images per class\", y=1.02);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Change the categories from numbers to names\n",
    "df[\"category\"] = df[\"category\"].replace(categories) \n",
    "\n",
    "# We first split the data into two sets and then split the validate_df to two sets\n",
    "train_df, validate_df = train_test_split(df, test_size=0.2, random_state=42)\n",
    "validate_df, test_df = train_test_split(validate_df, test_size=0.3, random_state=42)\n",
    "\n",
    "train_df = train_df.reset_index(drop=True)\n",
    "validate_df = validate_df.reset_index(drop=True)\n",
    "test_df = test_df.reset_index(drop=True)\n",
    "\n",
    "total_train = train_df.shape[0]\n",
    "total_validate = validate_df.shape[0]\n",
    "\n",
    "print('train size = ', total_validate , 'validate size = ', total_validate, 'test size = ', test_df.shape[0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGE_WIDTH = 224    \n",
    "IMAGE_HEIGHT = 224\n",
    "IMAGE_SIZE=(IMAGE_WIDTH, IMAGE_HEIGHT)\n",
    "IMAGE_CHANNELS = 3"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "batch_size=64\n",
    "\n",
    "train_datagen = ImageDataGenerator(\n",
    "    rotation_range=30,\n",
    "    shear_range=0.1,\n",
    "    zoom_range=0.3,\n",
    "    horizontal_flip=True,\n",
    "    vertical_flip=True,\n",
    "    width_shift_range=0.2,\n",
    "    height_shift_range=0.2\n",
    ")\n",
    "\n",
    "\n",
    "train_generator = train_datagen.flow_from_dataframe(\n",
    "    train_df, \n",
    "    data_path, \n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator(rescale=1./255)\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    test_df, \n",
    "    data_path, \n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "validation_datagen = ImageDataGenerator()\n",
    "\n",
    "validation_generator = validation_datagen.flow_from_dataframe(\n",
    "    validate_df, \n",
    "    data_path, \n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    class_mode='categorical',\n",
    "    batch_size=batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class_names = train_df.category.unique()\n",
    "print(class_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(15,15))\n",
    "for i in range(9):\n",
    "    random_row = random.randint(0, len(df)-1)\n",
    "    sample = df.iloc[random_row]\n",
    "    random_image = tf.keras.utils.load_img(data_path + sample['filename'])\n",
    "    plt.subplot(330 + 1 + i)\n",
    "    plt.title(sample['category'])\n",
    "    plt.imshow(random_image)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.figure(figsize=(10, 5))\n",
    "sns.countplot(x=\"category\", data=df, palette='Blues')\n",
    "plt.xticks(rotation=90)\n",
    "plt.title('Categories')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create the base model from the pre-trained model MobileNet V2\n",
    "IMG_SHAPE = (IMAGE_HEIGHT, IMAGE_WIDTH, 3)\n",
    "base_model = tf.keras.applications.VGG16(input_shape = IMG_SHAPE,\n",
    "                                         include_top = False,\n",
    "                                         weights = 'imagenet')\n",
    "#base_model.trainable = False\n",
    "base_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "def print_layer_trainable():\n",
    "    for layer in base_model.layers:\n",
    "        print(\"{0}:\\t{1}\".format(layer.trainable, layer.name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_layer_trainable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_model.trainable = False\n",
    "for layer in base_model.layers:\n",
    "    layer.trainable = False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print_layer_trainable()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data augmentation layer\n",
    "data_augmentation = keras.Sequential([\n",
    "    keras.layers.RandomFlip('horizontal', input_shape=(IMAGE_HEIGHT, IMAGE_WIDTH, 3)),\n",
    "    keras.layers.RandomRotation(0.2, fill_mode='nearest'),\n",
    "    keras.layers.RandomZoom(0.1),\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "n_classes = len(class_names)\n",
    "\n",
    "model = tf.keras.Sequential([\n",
    "    data_augmentation,\n",
    "    layers.Rescaling(1./255),\n",
    "    layers.Conv2D(32, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(64, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Conv2D(128, 3, activation='relu'),\n",
    "    layers.MaxPooling2D(),\n",
    "    layers.Flatten(),\n",
    "    layers.Dense(128, activation='relu'),\n",
    "    layers.Dropout(0.5),\n",
    "    layers.Dense(n_classes, activation='softmax')\n",
    "])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.compile(optimizer=tf.keras.optimizers.Adam(learning_rate=0.001), \n",
    "              loss='categorical_crossentropy', \n",
    "              metrics=['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Model Chackpoint\n",
    "tl_checkpoint_1 = ModelCheckpoint(filepath = 'vgg16_best_weights.keras', save_best_only = True, verbose = 0)\n",
    "\n",
    "# EarlyStopping\n",
    "early_stop = EarlyStopping(monitor = 'val_loss', patience = 5, restore_best_weights = True, mode = 'min')\n",
    "\n",
    "#ReduceLROnPlateau to stabilize the training process of the model\n",
    "rop_callback = ReduceLROnPlateau(monitor = 'val_loss', patience = 3, verbose = 1, factor = 0.5, min_lr = 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "history = model.fit(train_generator,\n",
    "                    epochs = 50,\n",
    "                    validation_data = validation_generator,\n",
    "                    callbacks = [tl_checkpoint_1, early_stop, rop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Ambil jumlah epochs dari history\n",
    "epochs_range = range(len(history.history['accuracy']))\n",
    "\n",
    "plt.figure(figsize=(20, 8))\n",
    "\n",
    "# Plot Training and Validation Accuracy\n",
    "plt.subplot(1, 2, 1)\n",
    "plt.plot(epochs_range, history.history['accuracy'], label='Training Accuracy')\n",
    "plt.plot(epochs_range, history.history['val_accuracy'], label='Validation Accuracy')\n",
    "plt.legend(loc='lower right')\n",
    "plt.title('Training and Validation Accuracy')\n",
    "\n",
    "# Plot Training and Validation Loss\n",
    "plt.subplot(1, 2, 2)\n",
    "plt.plot(epochs_range, history.history['loss'], label='Training Loss')\n",
    "plt.plot(epochs_range, history.history['val_loss'], label='Validation Loss')\n",
    "plt.legend(loc='upper right')\n",
    "plt.title('Training and Validation Loss')\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "test_datagen = ImageDataGenerator()\n",
    "\n",
    "test_generator = test_datagen.flow_from_dataframe(\n",
    "    dataframe= test_df,\n",
    "    directory=data_path,\n",
    "    x_col='filename',\n",
    "    y_col='category',\n",
    "    target_size=IMAGE_SIZE,\n",
    "    color_mode=\"rgb\",\n",
    "    class_mode=\"categorical\",\n",
    "    batch_size=1,\n",
    "    shuffle=False \n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "filenames = test_generator.filenames\n",
    "nb_samples = len(filenames)\n",
    "\n",
    "_, accuracy = model.evaluate(test_generator, steps=nb_samples // test_generator.batch_size)\n",
    "\n",
    "print('Accuracy on test set = ', round((accuracy * 100), 2), '%')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "gen_label_map = test_generator.class_indices\n",
    "gen_label_map = dict((v,k) for k,v in gen_label_map.items())\n",
    "print(gen_label_map)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import confusion_matrix, classification_report\n",
    "import seaborn as sns\n",
    "\n",
    "y_pred = model.predict(test_generator)\n",
    "y_pred_classes = np.argmax(y_pred, axis=1)\n",
    "y_true = test_generator.classes\n",
    "\n",
    "cm = confusion_matrix(y_true, y_pred_classes)\n",
    "sns.heatmap(cm, annot=True, fmt='d', cmap='Blues', xticklabels=class_names, yticklabels=class_names)\n",
    "plt.xlabel('Predicted')\n",
    "plt.ylabel('True')\n",
    "plt.show()\n",
    "\n",
    "print(classification_report(y_true, y_pred_classes, target_names=class_names))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save('garbage_model.h5')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_tune  = base_model\n",
    "# fine_tune.trainable = True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for layer in fine_tune.layers:\n",
    "#     # Boolean whether this layer is trainable.\n",
    "#     trainable = ('block5' in layer.name or 'block4' in layer.name)\n",
    "    \n",
    "#     # Set the layer's bool.\n",
    "#     layer.trainable = trainable"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fine_tune.summary()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "# n_classes = len(class_names)\n",
    "\n",
    "# model2 = Sequential([\n",
    "#     data_augmentation,\n",
    "#     keras.layers.Rescaling(1./255),\n",
    "#     fine_tune,\n",
    "#     keras.layers.GlobalAveragePooling2D(),\n",
    "#     keras.layers.Dense(128, activation = 'relu'),\n",
    "#     keras.layers.Dropout(0.5),\n",
    "#     keras.layers.Dense(n_classes, activation = 'softmax')\n",
    "# ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "# model2.compile(optimizer = tf.keras.optimizers.Adam(learning_rate = 0.001), \n",
    "#               loss = 'categorical_crossentropy', \n",
    "#               metrics = ['accuracy'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Model Chackpoint\n",
    "# tl_checkpoint_1 = ModelCheckpoint(filepath = 'vgg16_best_weights_fine_tuning.hdf5', save_best_only = True, verbose = 0)\n",
    "\n",
    "# # EarlyStopping\n",
    "# early_stop = EarlyStopping(monitor = 'val_loss', patience = 10, restore_best_weights = True, mode = 'min')\n",
    "\n",
    "# #ReduceLROnPlateau to stabilize the training process of the model\n",
    "# rop_callback = ReduceLROnPlateau(monitor = 'val_loss', patience = 3, verbose = 1, factor = 0.5, min_lr = 0.000001)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %%time\n",
    "# history = model2.fit(train_generator,\n",
    "#                     epochs = 20,\n",
    "#                     validation_data = validation_generator,\n",
    "#                     callbacks = [tl_checkpoint_1, early_stop, rop_callback])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import tensorflow as tf\n",
    "# new_model = tf.keras.models.load_model('model_garbage.keras')\n",
    "\n",
    "# # Show the model architecture\n",
    "# new_model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
